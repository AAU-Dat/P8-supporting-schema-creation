%! Author = Runge
%! Date = 29-12-2023

@book{goossens1993,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The LaTeX Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts"
}

@article{greenwade1993,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}


@article{mastroeni_abstract_2011,
    title = {An abstract interpretation-based model for safety semantics},
    volume = {88},
    issn = {0020-7160},
    url = {https://doi.org/10.1080/00207161003703205},
    doi = {10.1080/00207161003703205},
    abstract = {In this paper, we describe safety semantics as abstract interpretation of a trace-based operational semantics of a transition system. Intuitively, a property is safety if ‘nothing bad will happen’. Formally this is described by saying that a property is safety if it is maximal with respect to a given set of allowed partial executions. We show that this can be specified in the standard Cousot's framework of abstract interpretation. In particular, we show that this semantics can be derived as fixpoint of a semantic operator. This construction provides a formal characterization of the constructive nature of safety properties, that can be enforced by means of execution monitors. By using the same construction, we show that while safety without stuttering preserves the constructive nature, safety properties allowing cancellation of states lose the constructive characterization. Finally, we characterize safety properties as the closed elements of a closure, and we show that in the abstract interpretation framework safety and liveness properties lose their complementary nature.},
    number = {4},
    urldate = {2024-03-13},
    journal = {International Journal of Computer Mathematics},
    author = {Mastroeni, Isabella and Giacobazzi, Roberto},
    month = mar,
    year = {2011},
    keywords = {06B35, 68Q55, 68Q70, abstract interpretation, closure operators, program verification, safety, semantics},
    pages = {665--694},
}

@article{mine_static_2023,
    title = {Static analysis by abstract interpretation of concurrent programs},
    url = {https://theses.hal.science/tel-00903447},
    urldate = {2024-03-11},
    author = {Miné, Antoine},
    month = nov,
    year = {2023},
}

@article{gustafsson_analyzing_2013,
    title = {Analyzing {Execution}-{Time} of {Object}-{Oriented} {Programs} {Using} {Abstract} {Interpretation}},
    language = {English},
    urldate = {2024-03-11},
    author = {Gustafsson, Jan},
    month = dec,
    year = {2013},
}

@inproceedings{kashyap_integrity_2022,
    title = {Integrity {Constraint} {Verification} of {Structured} {Query} {Language} by {Abstract} {Interpretation}},
    url = {https://ieeexplore.ieee.org/abstract/document/10053712},
    doi = {10.1109/OCIT56763.2022.00091},
    abstract = {Over the decades and now-a-days the data-driven applications are playing a pivotal role in every aspect of our daily lives by providing an easy interface to store, access and process crucial data with the help of Database Management System (DBMS). However, it is always necessary to ensure the data integrity for every operation on a database. In this paper, we propose a novel framework for Structured Query Language (SQL), aiming at automatically and formally verifying integrity constraints in terms of enterprise policy specifications on data in the underlying database. To this aim, we extend the abstract interpretation theory to the case of structured query languages.},
    urldate = {2024-03-11},
    booktitle = {2022 {OITS} {International} {Conference} on {Information} {Technology} ({OCIT})},
    author = {Kashyap, Anwesha and Jana, Angshuman},
    month = dec,
    year = {2022},
    keywords = {Abstract Interpretation, Business, Data integrity, Database systems, Information technology, Proposals, SQL, Semantics, Structured Query Language, Verification},
    pages = {1--6},
}

@book{ganter_conceptual_2016,
    address = {Berlin, Heidelberg},
    title = {Conceptual {Exploration}},
    isbn = {978-3-662-49290-1 978-3-662-49291-8},
    url = {http://link.springer.com/10.1007/978-3-662-49291-8},
    language = {en},
    urldate = {2024-03-11},
    publisher = {Springer},
    author = {Ganter, Bernhard and Obiedkov, Sergei},
    year = {2016},
    doi = {10.1007/978-3-662-49291-8},
    keywords = {Data science, Formal concept analysis, Implication theories, Knowledge Acquisition, Mathematical method},
}

@article{dams_abstract_1997,
    title = {Abstract interpretation of reactive systems},
    volume = {19},
    issn = {0164-0925, 1558-4593},
    url = {https://dl.acm.org/doi/10.1145/244795.244800},
    doi = {10.1145/244795.244800},
    language = {en},
    number = {2},
    urldate = {2024-03-07},
    journal = {ACM Transactions on Programming Languages and Systems},
    author = {Dams, Dennis and Gerth, Rob and Grumberg, Orna},
    month = mar,
    year = {1997},
    pages = {253--291},
}

@inproceedings{cousot_abstract_1977,
    address = {Los Angeles, California},
    title = {Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
    shorttitle = {Abstract interpretation},
    url = {http://portal.acm.org/citation.cfm?doid=512950.512973},
    doi = {10.1145/512950.512973},
    language = {en},
    urldate = {2024-02-29},
    booktitle = {Proceedings of the 4th {ACM} {SIGACT}-{SIGPLAN} symposium on {Principles} of programming languages  - {POPL} '77},
    publisher = {ACM Press},
    author = {Cousot, Patrick and Cousot, Radhia},
    year = {1977},
    keywords = {Abstract Interpretation, Casper - first pass, Main},
    pages = {238--252},
}

@article{cousot_abstract_1996,
    title = {Abstract interpretation},
    volume = {28},
    issn = {0360-0300, 1557-7341},
    url = {https://dl.acm.org/doi/10.1145/234528.234740},
    doi = {10.1145/234528.234740},
    language = {en},
    number = {2},
    urldate = {2024-02-29},
    journal = {ACM Computing Surveys},
    author = {Cousot, Patrick},
    month = jun,
    year = {1996},
    keywords = {Abstract Interpretation, Casper - second pass, Main},
    pages = {324--328},
}

@book{nielson_formal_2019,
    address = {Cham},
    title = {Formal {Methods}: {An} {Appetizer}},
    isbn = {978-3-030-05155-6 978-3-030-05156-3},
    shorttitle = {Formal {Methods}},
    url = {http://link.springer.com/10.1007/978-3-030-05156-3},
    language = {en},
    urldate = {2024-02-23},
    publisher = {Springer International Publishing},
    author = {Nielson, Flemming and Riis Nielson, Hanne},
    year = {2019},
    doi = {10.1007/978-3-030-05156-3},
    keywords = {Main},
}

@book{moller_statitc_nodate,
    title = {Statitc {Program} {Analysis}},
    language = {en},
    author = {Møller, Anders and Schwartzbach, Michael I},
    keywords = {Abstract Interpretation, Main},
}

@misc{noauthor_abstract_nodate,
    title = {Abstract {Interpretation} in a {Nutshell}},
    url = {https://www.di.ens.fr/~cousot/AI/IntroAbsInt.html},
    urldate = {2024-02-27},
    keywords = {Abstract Interpretation, Casper - second pass, Main},
}

@article{jana_extending_2020,
    title = {Extending {Abstract} {Interpretation} to {Dependency} {Analysis} of {Database} {Applications}},
    volume = {46},
    issn = {0098-5589, 1939-3520, 2326-3881},
    url = {https://ieeexplore.ieee.org/document/8423692/},
    doi = {10.1109/TSE.2018.2861707},
    abstract = {Dependency information (data- and/or control-dependencies) among program variables and program statements is playing crucial roles in a wide range of software-engineering activities, e.g., program slicing, information ﬂow security analysis, debugging, code-optimization, code-reuse, code-understanding. Most existing dependency analyzers focus on mainstream languages and they do not support database applications embedding queries and data-manipulation commands. The ﬁrst extension to the languages for relational database management systems, proposed by Willmor et al. in 2004, suffers from the lack of precision in the analysis primarily due to its syntax-based computation and ﬂow insensitivity. Since then no signiﬁcant contribution is found in this research direction. This paper extends the Abstract Interpretation framework for static dependency analysis of database applications, providing a semantics-based computation tunable with respect to precision. More speciﬁcally, we instantiate dependency computation by using various relational and non-relational abstract domains, yielding to a detailed comparative analysis with respect to precision and efﬁciency. Finally, we present a prototype semDDA, a semantics-based Database Dependency Analyzer integrated with various abstract domains, and we present experimental evaluation results to establish the effectiveness of our approach. We show an improvement of the precision on an average of 6 percent in the interval, 11 percent in the octagon, 21 percent in the polyhedra and 7 percent in the powerset of intervals abstract domains, as compared to their syntax-based counterpart, for the chosen set of Java Server Page (JSP)-based open-source database-driven web applications as part of the GotoCode project.},
    language = {en},
    number = {5},
    urldate = {2024-03-04},
    journal = {IEEE Transactions on Software Engineering},
    author = {Jana, Angshuman and Halder, Raju and Abhishekh, Kalahasti Venkata and Ganni, Sanjeevini Devi and Cortesi, Agostino},
    month = may,
    year = {2020},
    pages = {463--494},
}

@misc{schuler_abstract_2023,
    title = {Abstract {Domains} for {Database} {Manipulating} {Processes}},
    url = {http://arxiv.org/abs/2308.03466},
    abstract = {Database manipulating systems (DMS) formalize operations on relational databases like adding new tuples or deleting existing ones. To ensure sufficient expressiveness for capturing practical database systems, DMS operations incorporate guarding expressions first-order formulas over countable value domains. Those features impose infinite state, infinitely branching processes thus making automated reasoning about properties like the reachability of states intractable. Most recent approaches, therefore, restrict DMS to obtain decidable fragments. Nevertheless, a comprehensive semantic framework capturing full DMS, yet incorporating effective notions of data abstraction and process equivalence is an open issue. In this paper, we propose DMS process semantics based on principles of abstract interpretation. The concrete domain consists of all valid databases, whereas the abstract domain employs different constructions for unifying sets of databases being semantically equivalent up to particular fragments of the DMS guard language. The connection between abstract and concrete domains is effectively established by homomorphic mappings whose properties and restrictions depend on the expressiveness of the DMS fragment under consideration. We instantiate our framework for canonical DMS fragments and investigate semantical preservation of abstractions up to bisimilarity, being one of the strongest equivalence notions for operational process semantics.},
    urldate = {2024-02-27},
    publisher = {arXiv},
    author = {Schüler, Tobias and Mennicke, Stephan and Lochau, Malte},
    month = aug,
    year = {2023},
    note = {arXiv:2308.03466 [cs]},
    keywords = {Abstract Interpretation, Computer Science - Databases},
}

@inproceedings{cortesi_abstract_2013,
    address = {Berlin, Heidelberg},
    series = {Lecture {Notes} in {Computer} {Science}},
    title = {Abstract {Interpretation} of {Recursive} {Queries}},
    isbn = {978-3-642-36071-8},
    doi = {10.1007/978-3-642-36071-8_12},
    abstract = {In this paper, we extend recent works on concrete and abstract semantics of structured query languages by considering recursive queries too. We show that combining abstraction of data and widening operators that guarantee the convergence of the computation may be useful not only for static analysis purposes, but also as a sound and effective tool for query language transformations.},
    language = {en},
    booktitle = {Distributed {Computing} and {Internet} {Technology}},
    publisher = {Springer},
    author = {Cortesi, Agostino and Halder, Raju},
    editor = {Hota, Chittaranjan and Srimani, Pradip K.},
    year = {2013},
    keywords = {Abstract Interpretation, Databases, Recursive Queries, Widening Operators},
    pages = {157--170},
}

@article{halder_abstract_2012,
    title = {Abstract interpretation of database query languages},
    volume = {38},
    issn = {1477-8424},
    url = {https://www.sciencedirect.com/science/article/pii/S1477842411000510},
    doi = {10.1016/j.cl.2011.10.004},
    abstract = {In this paper, we extend the Abstract Interpretation framework to the field of query languages for relational databases as a way to support sound approximation techniques. This way, the semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.},
    number = {2},
    urldate = {2024-02-27},
    journal = {Computer Languages, Systems \& Structures},
    author = {Halder, Raju and Cortesi, Agostino},
    month = jul,
    year = {2012},
    keywords = {Abstract Interpretation, Casper - first pass, Databases, Program analysis, SQL},
    pages = {123--157},
}

@inproceedings{vassiliadis_profiles_2021,
    address = {Chania, Greece},
    title = {Profiles of {Schema} {Evolution} in {Free} {Open} {Source} {Software} {Projects}},
    isbn = {978-1-72819-184-3},
    url = {https://ieeexplore.ieee.org/document/9458666/},
    doi = {10.1109/ICDE51399.2021.00008},
    abstract = {In this paper, we present the ﬁndings of a large study of the evolution of the schema of 195 Free Open Source Software projects. We identify families of evolutionary behaviors, or taxa, in FOSS projects. A large percentage of the projects demonstrate very few, if any, actions of schema evolution. Two other taxa involve the evolution via focused actions, with either a single focused maintenance action, or a large percentage of evolution activity grouped in no more than a couple interventions. Schema evolution also involves moderate, and active evolution, with very different volumes of updates to the schema. To the best of our knowledge, this is the ﬁrst study of this kind in the area of schema evolution, both in terms of presenting proﬁles of how schemata evolve, and, in terms of the dataset magnitude and the generalizability of the ﬁndings.},
    language = {en},
    urldate = {2024-02-26},
    booktitle = {2021 {IEEE} 37th {International} {Conference} on {Data} {Engineering} ({ICDE})},
    publisher = {IEEE},
    author = {Vassiliadis, Panos},
    month = apr,
    year = {2021},
    keywords = {Casper - first pass, Gabriela recommendation},
    pages = {1--12},
}

@article{sjoberg_quantifying_1993,
    title = {Quantifying schema evolution},
    volume = {35},
    issn = {09505849},
    url = {https://linkinghub.elsevier.com/retrieve/pii/095058499390027Z},
    doi = {10.1016/0950-5849(93)90027-Z},
    abstract = {Achieving correct changes is the dominant activity in the application software industry. Modification of database schemata is one kind of change which may have severe consequences for database applications. The paper presents a method for measuring modifications to database schemata and their consequences, by using a thesaurus tool. Measurements of the evolution of a large-scale database application currently running in several hospitals in the UK are presented and interpreted. The kind of measurements provided by this in-depth study is useful input to the design of change management tools.},
    language = {en},
    number = {1},
    urldate = {2024-02-26},
    journal = {Information and Software Technology},
    author = {Sjøberg, D},
    month = jan,
    year = {1993},
    keywords = {Casper - first pass, Gabriela recommendation},
    pages = {35--44},
}

@article{de_vries_case_2007,
    title = {The case for mesodata: {An} empirical investigation of an evolving database system},
    volume = {49},
    issn = {0950-5849},
    shorttitle = {The case for mesodata},
    url = {https://www.sciencedirect.com/science/article/pii/S0950584906001881},
    doi = {10.1016/j.infsof.2006.11.001},
    abstract = {Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute’s specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system.},
    number = {9},
    urldate = {2024-02-26},
    journal = {Information and Software Technology},
    author = {de Vries, Denise and Roddick, John F.},
    month = sep,
    year = {2007},
    keywords = {Casper - first pass, Database evolution, Domain evolution, Gabriela recommendation, Mesodata},
    pages = {1061--1072},
}

@article{weithoner_storing_nodate,
    title = {Storing and {Querying} {Ontologies} in {Logic} {Databases}},
    abstract = {The intersection of Description Logic inspired ontology languages with Logic Programs has been recently analyzed in [GHVD03]. The resulting language, called Description Logic Programs, covers RDF Schema and a notable portion of OWL Lite. However, the proposed mapping in [GHVD03] from the corresponding OWL fragment into Logic Programs has shown scalability as well as representational deﬁcits within our experiments and analysis. In this paper we propose an alternative mapping resulting in lower computational complexity and more representational ﬂexibility. We also present benchmarking results for both mappings with ontologies of diﬀerent size and complexity.},
    language = {en},
    author = {Weithoner, Timo and Liebig, Thorsten and Specht, Gunther},
}

@inproceedings{beeri_equivalence_1979,
    address = {Atlanta, Georgia, United States},
    title = {Equivalence of relational database schemes},
    url = {http://portal.acm.org/citation.cfm?doid=800135.804424},
    doi = {10.1145/800135.804424},
    language = {en},
    urldate = {2024-02-21},
    booktitle = {Proceedings of the eleventh annual {ACM} symposium on {Theory} of computing  - {STOC} '79},
    publisher = {ACM Press},
    author = {Beeri, Catriel and Mendelzon, Alberto O. and Sagiv, Yehoshua and Ullman, Jeffrey D.},
    year = {1979},
    pages = {319--329},
}

@article{fiadeiro_specification_1988,
    title = {Specification and verification of database dynamics},
    volume = {25},
    issn = {0001-5903, 1432-0525},
    url = {http://link.springer.com/10.1007/BF00291052},
    doi = {10.1007/BF00291052},
    abstract = {A framework is proposed for the structured specification and verification of database dynamics. In this framework, the conceptual model of a database is a many sorted first order linear tense theory whose proper axioms specify the update and the triggering behaviour of the database. The use of conceptual modelling approaches for structuring such a theory is analysed. Semantic primitives based on the notions of event and process are adopted for modelling the dynamic aspects. Events are used to model both atomic database operations and communication actions (input/output). Nonatomic operations to be performed on the database (transactions) are modelled by processes in terms of trigger/reaction patterns of behaviour. The correctness of the specification is verified by proving that the desired requirements on the evolution of the database are theorems of the conceptual model. Besides the traditional data integrity constraints, requirements of the form "Under condition W, it is guaranteed that the database operation Z will be successfully performed" are also considered. Such liveness requirements have been ignored in the database literature, although they are essential to a complete definition of the database dynamics.},
    language = {en},
    number = {6},
    urldate = {2024-02-19},
    journal = {Acta Informatica},
    author = {Fiadeiro, Jos� and Sernadas, Am�lcar},
    month = aug,
    year = {1988},
    pages = {625--661},
}

@article{le_formal_2001,
    title = {Formal {Analysis} of {Database} {Trigger} {Systems} {Using} {Event}-{B}},
    copyright = {Access limited to members},
    issn = {2166-7160},
    url = {https://www.igi-global.com/article/formal-analysis-of-database-trigger-systems-using-event-b/268330},
    language = {English},
    urldate = {2024-02-19},
    journal = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJSI.20211001.oa1},
    author = {Le, Anh Hong and Van Khanh, To and Thuan, Truong Ninh},
    month = jan,
    year = {2001},
    note = {Publisher: IGI Global},
}

@article{dillig_formal_nodate,
    title = {Formal {Methods} for {Database} {Schema} {Refactoring}},
    language = {en},
    author = {Dillig, Isil and Cook, William and Pillai, Vijaychidambaram Velayudhan and Solar-Lezama, Armando and Jermaine, Christopher},
    keywords = {Casper - first pass},
}

@incollection{javed_automated_2020,
    address = {Cham},
    series = {Studies in {Computational} {Intelligence}},
    title = {Automated {Assessment} of {ER} {Model} {Using} the {Domain} {Knowledge}},
    isbn = {978-3-030-25213-7},
    url = {https://doi.org/10.1007/978-3-030-25213-7_10},
    abstract = {It is a challenging task to develop a complete and correct Entity Relationship Model (ERM) from the requirements. Quality of artifacts in the later stages of software development (e.g. logical database design, physical database design, and the final product) dependents on the conceptual models. Domain Knowledge (DK) plays a key role while extracting the artifacts from requirements. It is agreed upon that most errors in the early stages of software development are due to the lack of adequate DK. In this paper, we are proposing an automated assessment approach, which focuses on some major issues of ERM such as completeness and correctness. The automatically extracted ERM is used as a reference for the assessment of a manually developed model from the same set of requirements. We trained the Skip-gram model of word2vec for extracting the DK, which is used for assisting in errors detection and ERM’s labels matching. As a case study, we considered models from the business domain. Inputs of this automated technique are reference model, DK, and the model to be evaluated. The output is a list of errors (indicating the sources) and suggestions. The results show the proposed approach is having a noticeable improvement over the existing approaches.},
    language = {en},
    urldate = {2024-02-14},
    booktitle = {Computer and {Information} {Science}},
    publisher = {Springer International Publishing},
    author = {Javed, Muhammad and Lin, Yuqing},
    editor = {Lee, Roger},
    year = {2020},
    doi = {10.1007/978-3-030-25213-7_10},
    keywords = {Automatic quality assessment, Casper - first pass, Conceptual diagram, Domain knowledge, Entity relationship model, Gabriela recommendation},
    pages = {143--162},
}

@book{thonggoom_semi-automatic_2011,
    title = {Semi-automatic {Conceptual} {Data} {Modeling} {Using} {Entity} and {Relationship} {Instance} {Repositories}},
    volume = {6998},
    isbn = {978-3-642-24605-0},
    abstract = {Data modelers frequently lack experience and have incomplete knowledge about the application being designed. To address this issue, we propose new types of reusable artifacts called Entity Instance Repository (EIR) and Relationship Instance Repository (RIR), which contain ER modeling patterns from prior designs and serve as knowledge-based repositories for conceptual modeling. We explore the development of automated data modeling tools with EIR and RIR. We also select six data modeling rules used for identification of entities in one of the tools. Two tools were developed in this study: Heuristic-Based Technique (HBT) and Entity Instance Pattern WordNet (EIPW). The goals of this study are (1) to find effective approaches that can improve the novice modelers’ performance in developing conceptual models by integrating pattern-based technique and various modeling techniques, (2) to evaluate whether those selected six modeling rules are effective, and (3) to validate whether the proposed tools are effective in creating quality data models. In order to evaluate the effectiveness of the tools, empirical testing was conducted on tasks of different sizes. The empirical results indicate that novice designers’ overall performance increased 30.9{\textasciitilde}46.0\% when using EIPW, and increased 33.5{\textasciitilde}34.9 \% when using HBT, compared with the cases with no tools.},
    author = {Thonggoom, Ornsiri and Song, Il-Yeol and An, Yuan},
    month = oct,
    year = {2011},
    doi = {10.1007/978-3-642-24606-7_17},
    note = {Pages: 232},
    keywords = {Casper - first pass, Gabriela recommendation},
}

@article{storey_semantic_1996,
    title = {Semantic integrity constraints in knowledge-based database design systems},
    volume = {20},
    issn = {0169023X},
    url = {https://linkinghub.elsevier.com/retrieve/pii/0169023X9500035Q},
    doi = {10.1016/0169-023X(95)00035-Q},
    abstract = {Semantic integrity constraints are used to help ensure that a database accurately reflects the real world in structure and content. Existing database design methodologies, however, do not often include a comprehensive procedure for identifying such constraints and incorporating them into a design. A number of knowledge-based systems for database design have been developed with varying degrees of emphasis on, and approaches to, this task. A representative sample of these systems is reviewed for their ability to identify, represent and validate integrity constraints.},
    language = {en},
    number = {1},
    urldate = {2024-02-14},
    journal = {Data \& Knowledge Engineering},
    author = {Storey, Veda C. and Yang, Heng-Li and Goldstein, Robert C.},
    month = jun,
    year = {1996},
    keywords = {Casper - first pass, Casper - second pass, Gabriela recommendation},
    pages = {1--37},
}

@article{rosenthal_tools_1994,
    title = {Tools and transformations—rigorous and otherwise—for practical database design},
    volume = {19},
    issn = {0362-5915, 1557-4644},
    url = {https://dl.acm.org/doi/10.1145/176567.176568},
    doi = {10.1145/176567.176568},
    abstract = {We describe the tools and theory of a comprehensive system for database design, and show how they work together to support multiple conceptual and logical design processes. The Database Design and Evaluation Workbench (DDEW) system uses a rigorous, information-content-preserving approach to schema transformation, but combines it with heuristics, guess work, and user interactions. The main contribution lies in illustrating how theory was adapted to a practical system, and how the consistency and power of a design system can be increased by use of theory.
            First, we explain why a design system needs multiple data models, and how implementation over a unified underlying model reduces redundancy and inconsistency. Second, we present a core set of small but fundamental algorithms   that reaarange a schema without changing its information content. From these reusable components, we easily built larger tools and transformations that were still formally justified. Third, we describe heuristic tools that attempt to improve a schema, often by adding missing information. In these tools, unreliable techniques such as normalization and relationship inference are bolstered by system-guided user interactions to remove errors. We present a rigorous criterion for identifying unnecessary relationships, and discuss an interactive view integrator. Last, we examine the relevance of database theory to building these practically motivated tools and contrast the paradigms of system builders with those of theoreticians.},
    language = {en},
    number = {2},
    urldate = {2024-02-14},
    journal = {ACM Transactions on Database Systems},
    author = {Rosenthal, Arnon and Reiner, David},
    month = jun,
    year = {1994},
    keywords = {Casper - first pass, Gabriela recommendation},
    pages = {167--211},
}

@article{storey_understanding_1993,
    title = {Understanding semantic relationships},
    volume = {2},
    issn = {1066-8888, 0949-877X},
    url = {http://link.springer.com/10.1007/BF01263048},
    doi = {10.1007/BF01263048},
    abstract = {To develop sophisticated database management systems, there is a need to incorporate more understanding of the real world in the information that is stored in a database. Semantic data models have been developed to try to capture some of the meaning, as well as the structure, of data using abstractions such as inclusion, aggregation, and association. Besides these well-known relationships, a number of additional semantic relationships have been identified by researchers in other disciplines such as linguistics, logic, and cognitive psychology. This article explores some of the lesser-recognized semantic relationships and discusses both how they could be captured, either manually or by using an automated tool, and their impact on database design. To demonstrate the feasibility of this research, a prototype system for analyzing semantic relationships, called the Semantic Relationship Analyzer, is presented.},
    language = {en},
    number = {4},
    urldate = {2024-02-14},
    journal = {The VLDB Journal},
    author = {Storey, Vede C.},
    month = oct,
    year = {1993},
    keywords = {Casper - first pass, Gabriela recommendation},
    pages = {455--488},
}

@article{li_transactions_2023,
    title = {Transactions {Make} {Debugging} {Easy}},
    abstract = {We propose TROD, a novel transaction-oriented framework for debugging modern distributed web applications and online services. Our critical insight is that if applications store all state in databases and only access state transactionally, TROD can use lightweight always-on tracing to track the history of application state changes and data provenance, and then leverage the captured traces and transaction logs to faithfully replay or even test modified code retroactively on any past event. We demonstrate how TROD can simplify programming and debugging in production applications, list several research challenges and directions, and encourage the database and systems communities to drastically rethink the synergy between the way people develop and debug applications.},
    language = {en},
    author = {Li, Qian and Kraft, Peter and Cafarella, Michael and Demiralp, Çağatay and Graefe, Goetz and Kozyrakis, Christos and Stonebraker, Michael and Suresh, Lalith and Zaharia, Matei},
    year = {2023},
    keywords = {CIDR 2023, Casper - first pass},
}

@article{floratou_nl2sql_2024,
    title = {{NL2SQL} is a solved problem... {Not}!},
    abstract = {The development of natural language (NL) interfaces for databases has been notably shaped by the rise of Large Language Models (LLMs), which provide an easy way to automate the translation of NL queries into structured SQL queries. While LLMs bring valuable technical advancements, this paper stresses that achieving Enterprise-Grade NL2SQL is still far from being resolved, necessitating extensive novel research in various domains. We present insights from two competing teams dedicated to delivering reliable enterprise-grade NL2SQL technology, shedding light on challenges faced in real-world applications, including handling complex schemata, dealing with ambiguity in natural language statements, and incorporating it in our benchmarking methodologies and responsible AI considerations. While this paper may raise more questions than it answers, its aim is to act as a catalyst for a fruitful discussion on the topic. Additionally, it provides a practical pathway for the community to develop enterprise-grade NL2SQL solutions.},
    language = {en},
    author = {Floratou, Avrilia and Psallidas, Fotis and Zhao, Fuheng and Deep, Shaleen and Hagleither, Gunther and Cahoon, Joyce and Alotaibi, Rana and Henkel, Jordan and Singla, Abhik and van Grootel, Alex and Deng, Kai and Lin, Katherine and Campos, Marcos and Emani, Venkatesh and Pandit, Vivek and Wang, Wenjing and Curino, Carlo},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{tang_verifai_2024,
    title = {{VerifAI}: {Verified} {Generative} {AI}},
    abstract = {Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and AI regulation practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach can ensure the correctness of generative AI, promote transparency, and enable decision-making with greater confidence. Our vision is to promote the development of verifiable generative AI and contribute to a more trustworthy and responsible use of AI.},
    language = {en},
    author = {Tang, Nan and Yang, Chenyu and Fan, Ju and Cao, Lei and Luo, Yuyu and Halevy, Alon},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{jindal_turning_2024,
    title = {Turning {Databases} {Into} {Generative} {AI} {Machines}},
    abstract = {Data is no more the commodity oil. Today, it is an asset for any enterprise. However, turning data into intelligence remains a challenge for most people. In this paper, we explore whether databases can be turned into generative AI machines that can talk to anyone. We identify three core challenges when applying generative AI on data, namely accuracy, scale, and privacy, and show how a generative large data model could solve all of these. We describe our conceptual framework of generative AI on databases, the GOD machine, and ground it in production workloads at SmartApps. Our results promise new directions in fusing AI with data.},
    language = {en},
    author = {Jindal, Alekh and Qiao, Shi and Madhula, Sathwik Reddy and Raheja, Kanupriya and Jain, Sandhya},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{rajan_welding_2024,
    title = {Welding {Natural} {Language} {Queries} to {Analytics} {IRs} with {LLMs}},
    abstract = {From the recent momentum behind translating natural language to SQL (nl2sql), to commercial product offerings such as Co-Pilot for Microsoft Fabric, Large Language Models (LLMs) are poised to have a big impact on data analytics. In this paper, we show that LLMs can be used to convert natural language analytics queries directly to custom intermediate query representations (IRs) of modern data analytics systems. This has the direct benefit of making IRs more accessible to end-users, but interestingly, it can also result in improved translation accuracy and better end-to-end performance, especially when the query semantics is better captured in the IR rather than in SQL. We build an LLM-based pipeline (nl2weld) for one instance of this flow, to translate natural language queries to the Weld IR using gpt-4. nl2weld is carefully designed to harness selfreflection and instruction-following capabilities of gpt-4, providing it various forms of feedback such as domain specific instructions and feedback from the Weld compiler. We evaluate nl2weld on a subset of the Spider benchmark and compare it against the gold standard SQL and DIN-SQL, a state-of-the-art nl2sql system. We report a comparable accuracy of 77.4\% on the dataset, and also demonstrate examples on which nl2weld produces code that is 1.2 − 3× faster than the gold standard and DIN-SQL.},
    language = {en},
    author = {Rajan, Kaushik and Rastogi, Aseem and Lal, Akash and Rajendra, Sampath and Subramanian, Krithika and Patel, Krut},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{zhang_cost-intelligent_nodate,
    title = {Cost-{Intelligent} {Data} {Analytics} in the {Cloud}},
    abstract = {For decades, database research has focused on optimizing performance under fixed resources. As more and more database applications move to the public cloud, we argue that it is time to make cost a first-class citizen when solving database optimization problems. In this paper, we introduce the concept of cost intelligence and envision the architecture of a cloud data warehouse designed for that. We investigate two critical challenges to achieving cost intelligence in an analytical system: automatic resource deployment and cost-oriented auto-tuning. We describe our system architecture with an emphasis on the components that are missing in today’s cloud data warehouses. Each of these new components represents unique research opportunities in this much-needed research area.},
    language = {en},
    author = {Zhang, Huanchen and Liu, Yihao and Yan, Jiaqi},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{franz_dear_2024,
    title = {Dear {User}-{Defined} {Functions},  {Inlining} isn't working out so great for us.  {Let}'s try batching to make our relationship work.  {Sincerely}, {SQL}},
    abstract = {SQL’s user-defined functions (UDFs) allow developers to express complex computation using procedural logic. But UDFs have been the bane of database management systems (DBMSs) for decades because they inhibit optimization opportunities, potentially slowing down queries significantly. In response, batching and inlining techniques have been proposed to enable effective query optimization of UDF calls within SQL. Inlining is now available in a major commercial DBMS. But the trade-offs between both approaches on modern DBMSs remain unclear.},
    language = {en},
    author = {Franz, Kai and Arch, Samuel and Hirn, Denis and Grust, Torsten and Mowry, Todd C and Pavlo, Andrew},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{durner_tracex_2024,
    title = {{TracEx}: {Understanding} and {Analyzing} {Database} {Traces}},
    abstract = {With the shift to databases-as-a-service, vendors are able to collect high-level database traces of executed workloads while retaining the privacy of their customers. In contrast to pure end-to-end latency statistics, traces contain enriched information that is useful for tasks such as workload monitoring and regression testing. Despite its importance, efficient analysis and exploration of traces and their rich feature space remains a challenge. In this paper, we introduce TracEx, an open-source Trace Exploration tool that facilitates workload trace analysis and comparison for database systems. TracEx allows users to understand their workload by providing an intuitive, visual interface that explores the workload along different dimensions, e.g., resource utilization or database operator usage. Additionally, users are able to contrast and compare workloads that have been collected from different hardware configurations or even compare traces between database systems.},
    language = {en},
    author = {Durner, Dominik},
    year = {2024},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{singh_panda_2018,
    title = {Panda: {Performance} {Debugging} for {Databases} using {LLM} {Agents}},
    abstract = {Debugging a performance issue in databases is notoriously hard. Wouldn’t it be convenient if there exists an oracle or a co-pilot for every database system which users can query in natural language (NL) — ‘what’s wrong?’, or even better— ‘How to fix it?’. Large Language Models (LLMs), like ChatGPT, seem to be a natural surrogate to this oracle given their ability to answer a wide range of questions by efficiently encoding vast amount of knowledge for e.g., a major chunk of the internet. However, prompting ChatGPT with database performance queries often results in ‘technically correct’ but highly ‘vague’ or ‘generic’ recommendations typically rendered useless and untrustworthy by experienced Database Engineers (DBEs).},
    language = {en},
    author = {Singh, Vikramank and Vaidya, Kapil Eknath and Bannihatti, Vinayshekhar and Khosla, Sopan and Narayanaswamy, Balakrishnan and Gangadharaiah, Rashmi and Kraska, Tim},
    year = {2018},
    keywords = {CIDR 2024, Casper - first pass},
}

@article{neumann_critique_nodate,
    title = {A {Critique} of {Modern} {SQL} {And} {A} {Proposal} {Towards} {A} {Simple} and {Expressive} {Query} {Language}},
    abstract = {The first contribution of the paper is a comprehensive critique of modern SQL, informed by an analysis of real-world SQL queries. This provides the motivation for our second contribution: the Simple ANd Expressive Query Language (SaneQL). SaneQL features a straightforward and consistent syntax, which improves its learnability and ease of implementation. Additionally, it provides extensibility, with the added ability to define new operators that integrate seamlessly with the existing built-in ones. Unlike most data frame APIs and NoSQL query languages, SaneQL fully embraces the core principles behind SQL, especially multiset semantics. We propose that adopting SaneQL’s approach can ensure the enduring success of relational database technology, offering the power of SQL’s underlying concepts through a more accessible and flexible language.},
    language = {en},
    author = {Neumann, Thomas and Leis, Viktor},
    keywords = {CIDR 2024, Casper - first pass},
}

